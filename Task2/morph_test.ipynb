{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7daaf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1abac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('wsmp_train.json')\n",
    "eval_data = load_data('wsmp_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e876163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ghaṭa stha yogam yoga īśa tattva jñānasya kāraṇam',\n",
       " [('ghaṭa', 'iic.'),\n",
       "  ('stha', 'iic.'),\n",
       "  ('yoga', 'm. sg. acc.'),\n",
       "  ('yoga', 'iic.'),\n",
       "  ('īśa', 'm. sg. voc.'),\n",
       "  ('tattva', 'iic.'),\n",
       "  ('jñāna', 'n. sg. g.'),\n",
       "  ('kāraṇa', 'n. sg. acc.')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd04d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "rules = defaultdict(int)\n",
    "\n",
    "\"\"\"\n",
    "Find rules = operations how to transform token -> stem\n",
    "\"\"\"\n",
    "\n",
    "for sentence, labels in data:\n",
    "    tokens = sentence.split()\n",
    "    stems, _ = zip(*labels)\n",
    "    \n",
    "    # Skip malformed data\n",
    "    if len(tokens) != len(stems):\n",
    "        continue\n",
    "    \n",
    "    # Find rule for each token, stem pair\n",
    "    for token, stem in zip(tokens, stems):\n",
    "        # Find possible starting indices for overlapping\n",
    "        # sequences for chars\n",
    "        indices = []\n",
    "        for i, char in enumerate(token):\n",
    "            if char == stem[0]:\n",
    "                indices.append(i)\n",
    "        \n",
    "        # If no overlap, no rule\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find length of overlapping char segments\n",
    "        match_lens = []\n",
    "        for idx in indices:\n",
    "            current_length = 0\n",
    "            for k in range(0, min(len(stem), len(token) - idx)):\n",
    "                if stem[k] == token[idx + k]:\n",
    "                    current_length += 1\n",
    "                else:\n",
    "                    break\n",
    "            match_lens.append(current_length)\n",
    "        \n",
    "        # Take longest overlapping char segment\n",
    "        # as 'root' (may be different from linguistic root)\n",
    "        best_idx = np.argmin(match_lens)\n",
    "        best_length = match_lens[best_idx]\n",
    "        best_idx = indices[best_idx]\n",
    "        \n",
    "        # If no overlap, no rule\n",
    "        if best_length == 0:\n",
    "            continue\n",
    "        \n",
    "        # Prefix = part before 'root'\n",
    "        prefix = token[:best_idx]\n",
    "        # Suffix = part after 'root'\n",
    "        suffix = token[best_idx + best_length:]\n",
    "        # Replaced suffix\n",
    "        stem_suffix = stem[best_length:]\n",
    "        \n",
    "        # Save rule\n",
    "        rules[(prefix, suffix, stem_suffix)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2a7eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_stem_pairs = set()\n",
    "\n",
    "for sentence, labels in data:\n",
    "    tokens = sentence.split()\n",
    "    stems, _ = zip(*labels)\n",
    "    \n",
    "    if len(tokens) != len(stems):\n",
    "        continue\n",
    "        \n",
    "    for token, stem in zip(tokens, stems):\n",
    "        token_stem_pairs.add((token, stem))\n",
    "\n",
    "token_stem_pairs = list(token_stem_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c597cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_token_stem_pairs = set()\n",
    "\n",
    "for sentence, labels in eval_data:\n",
    "    tokens = sentence.split()\n",
    "    stems, _ = zip(*labels)\n",
    "    \n",
    "    if len(tokens) != len(stems):\n",
    "        continue\n",
    "        \n",
    "    for token, stem in zip(tokens, stems):\n",
    "        eval_token_stem_pairs.add((token, stem))\n",
    "\n",
    "eval_token_stem_pairs = list(eval_token_stem_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "730c394d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20011"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_token_stem_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15379b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6dc8993e494ba69f9509cf7e865fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "success = 0\n",
    "\n",
    "valid_rules = [rule for rule, count in rules.items() if count > 5]\n",
    "candidate_lengths = []\n",
    "\n",
    "for token, stem in tqdm(eval_token_stem_pairs):\n",
    "    possible_stems = []\n",
    "    \n",
    "    # Reconstruct candidates\n",
    "    for prefix, suffix, stem_suffix in valid_rules:\n",
    "        possible_stem = token[:]\n",
    "        if possible_stem.startswith(prefix):\n",
    "            possible_stem = possible_stem[len(prefix):]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if possible_stem.endswith(suffix):\n",
    "            possible_stem = possible_stem[:len(possible_stem) - len(suffix)]\n",
    "            possible_stem += stem_suffix\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        possible_stems.append(possible_stem)\n",
    "    \n",
    "    #possible_stems = list(sorted(set(possible_stems)))\n",
    "    possible_stems = list(possible_stems)\n",
    "    \n",
    "    if stem in possible_stems:\n",
    "        success += 1\n",
    "    \n",
    "    candidate_lengths.append(len(possible_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5fc04eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc. of reconstructed stems: 0.9715656388986058\n",
      "Avg. number of candidate stems: 8.560091949427814\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perc. of reconstructed stems: {success / len(eval_token_stem_pairs)}\")\n",
    "print(f\"Avg. number of candidate stems: {np.mean(candidate_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "857d23d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aśāntau',\n",
       " 'aśāntaun',\n",
       " 'aśānta',\n",
       " 'aśāntad',\n",
       " 'aśānti',\n",
       " 'aśāntu',\n",
       " 'aśānt',\n",
       " 'aśāt',\n",
       " 'aśāntā']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "889e6583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2730"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "161e1a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maṇḍala', '', 'aṇḍala'),\n",
       " ('abhyās', '', 'bhyāsa'),\n",
       " ('', 'ena', 'a'),\n",
       " ('', 'yate', ''),\n",
       " ('', 'āyate', 'an'),\n",
       " ('', 't', 'd'),\n",
       " ('', '', 'n'),\n",
       " ('ādh', 'ra', 'dhāra'),\n",
       " ('', 'iḥ', ''),\n",
       " ('', 'cchet', 'm'),\n",
       " ('', 'uvoḥ', 'ū'),\n",
       " ('', 'yati', ''),\n",
       " ('', 'avati', 'ū'),\n",
       " ('āpnuy', 't', 'p'),\n",
       " ('', 'ā', 'an'),\n",
       " ('', 'āya', 'a'),\n",
       " ('mudrā', '', 'udrā'),\n",
       " ('', 'ayaḥ', 'i'),\n",
       " ('', 'anti', ''),\n",
       " ('amb', 'ra', 'mbara')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_rules[20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91665af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
